# -*- coding: utf-8 -*-
"""
Created on Fri Apr 14 12:18:11 2023

@author: jimmy
"""
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier
from sklearn.metrics import accuracy_score,precision_score, f1_score

f = pd.read_csv("Breast Cancer Wisconsin.csv")
df = f.drop([f.columns[0]], axis=1)

values = ['?']
df = df[df.isin(values) == False]
df = df.dropna()
df = df.reset_index(drop = True)

X = df.iloc[:, :-1]
y = df.iloc[:, -1]

np.random.seed(42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
# =============================================================================
#i. KNN
best_acc = -1
best_f1 = -1
best_k = None

print("i. KNN")

for k in range(1,16):
    model = KNeighborsClassifier(n_neighbors = k)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test.values, y_pred)
    f1 = f1_score(y_test.values, y_pred,pos_label=4)
    print("K:",k,",Accuracy:", acc)
    print("F1 score:",f1)

    
    if acc > best_acc and f1 > best_f1:
        best_acc = acc
        best_k = k
        best_f1 = f1
        
print("Best K:",best_k,",Accuracy:",best_acc)
print("F1 score:", best_f1)
# =============================================================================
#ii. Logistic Regression
print("\nii. Logistic Regression")

model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
acc = accuracy_score(y_test.values, y_pred)
f1 = f1_score(y_test.values, y_pred,pos_label=4)

print("Accuracy:", acc)
print("F1 score:",f1)

# =============================================================================
#iii. Decision Tree
print("\niii. Decision Tree")

best_acc = -1
best_f1 = -1
best_depth = None

for max_depth in range(1,11):
    model = DecisionTreeClassifier(max_depth = max_depth)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test.values, y_pred)
    f1 = f1_score(y_test.values, y_pred,pos_label=4)
    print("Max Number of Splits:",max_depth,",Accuracy:", acc)
    print("F1 score:",f1)

    if acc > best_acc and f1 > best_f1:
        best_acc = acc
        best_f1 = f1
        best_depth = max_depth
        
print("Best Max Number of Splits:",best_depth,",Accuracy:",best_acc)
print("F1 score:", best_f1)
# =============================================================================
#iv. SVM
print("\niv. SVM")

model = SVC(kernel='linear')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
acc = accuracy_score(y_test.values, y_pred)
f1 = f1_score(y_test.values, y_pred,pos_label=4)

print("Accuracy:", acc)
print("F1 score:",f1)
# =============================================================================
#v. Boosted Tree and Bagged Tree
print("\niv. Boosted Tree and Bagged Tree")

SVM = SVC(kernel='linear')
model = AdaBoostClassifier(base_estimator=SVM,algorithm='SAMME')
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
acc = accuracy_score(y_test.values, y_pred)
f1 = f1_score(y_test.values, y_pred,pos_label=4)

print("Boosted Tree")
print("Accuracy:", acc)
print("F1 score:",f1)

model = BaggingClassifier(base_estimator=SVM)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
acc = accuracy_score(y_test.values, y_pred)
f1 = f1_score(y_test.values, y_pred,pos_label=4)

print("Bagged Tree")
print("Accuracy:", acc)
print("F1 score:",f1)